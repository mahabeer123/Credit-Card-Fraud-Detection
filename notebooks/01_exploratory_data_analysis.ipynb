{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19960182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required packages\n",
    "\n",
    "#modelues for EDA steps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#modules for data cleaning and data analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "#modules for model building\n",
    "#algorithms for sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#baseline linear model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#modules for hyper parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#modules for model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, r2_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "#modules for avoiding warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting backend for matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "#setting formatting options\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 900\n",
    "pd.set_option('float_format' , '{:f}'.format)\n",
    "\n",
    "#setting plot style\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "#loading the dataset\n",
    "fraud_train = pd.read_csv('fraudTrain.csv')\n",
    "fraud_test = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "#concatenating the two datasets\n",
    "df = pd.concat([fraud_train, fraud_test]).reset_index()\n",
    "\n",
    "df.drop(df.columns[:2], axis=1, inplace=True)\n",
    "df.head()\n",
    "#converting trans_date_trans_time into datetime\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "print(df.dtypes['trans_date_trans_time'])\n",
    "df.head()\n",
    "#let us look at the number of unique values in the dataset\n",
    "df.nunique()\n",
    "# deriving additonal columns from 'trans_date_trans_time'\n",
    "#deriving hour\n",
    "df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "#deriving 'day of the week'\n",
    "df['trans_day_of_week'] = df['trans_date_trans_time'].dt.day_name()\n",
    "#deriving 'year_month'\n",
    "df['trans_year_month'] = df['trans_date_trans_time'].dt.to_period('M')\n",
    "\n",
    "df.head()\n",
    "#finding age\n",
    "#converting 'dob' column to datetime\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "df['age'] = np.round((df['trans_date_trans_time'] - \n",
    "                      df['dob'])/np.timedelta64(1, 'Y'))\n",
    "#dropping variables\n",
    "df.drop(['trans_date_trans_time','first', 'last', 'dob'] , axis=1, inplace=True)\n",
    "df.head()\n",
    "#checking the dataset info\n",
    "df.info()\n",
    "#storing a copy\n",
    "df_org = df.copy()\n",
    "#determing the shape of the dataset\n",
    "df.shape\n",
    "#checking the dataframe\n",
    "df.head()\n",
    "#describing the dataset\n",
    "df.describe()\n",
    "#let us check the percentage of fraudulent data points in our dataset\n",
    "100*df.is_fraud.value_counts(normalize=True)\n",
    "#looking at distribution of amount\n",
    "pd.concat(\n",
    "    [df['amt'].describe(percentiles = [0.5,0.95,0.999]).reset_index().rename(columns={'index': 'Row Type', 'amt':'Overall Amt Distribution'}),\n",
    "     df.loc[df['is_fraud']==0,['amt']].describe(percentiles = [0.5,0.95,0.999]).reset_index(drop = 1).rename(columns={'amt':'Non-Fraud Amt Distribution'}),\n",
    "     df.loc[df['is_fraud']==1,['amt']].describe(percentiles = [0.5,0.95,0.999]).reset_index(drop = 1).rename(columns={'amt':'Fraud Amt Distribution'})], axis=1)\n",
    "#plotting the above distributions\n",
    "fig = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plots = []\n",
    "#plotting the amt feature\n",
    "#box plot\n",
    "plots.append(sns.boxplot(df.amt, ax=plt.subplot(211)))\n",
    "\n",
    "#distribution plots\n",
    "plots.append(sns.histplot(df[df.amt <= 1500].amt, bins=50, ax=plt.subplot(234)))\n",
    "plots.append(sns.histplot(df[(df.is_fraud==0) & (df.amt<=1500)].amt, bins=50, ax=plt.subplot(235)))\n",
    "plots.append(sns.histplot(df[(df.is_fraud==1) & (df.amt<=1500)].amt, bins=50, ax=plt.subplot(236)))\n",
    "\n",
    "#setting titles\n",
    "plots[1].set_title('Overall amt Dist')\n",
    "plots[2].set_title('Non Fraud amt Dist')\n",
    "plots[3].set_title('Fraud amt Dist')\n",
    "\n",
    "#setting x labels\n",
    "plots[1].set_xlabel('Transaction Amount')\n",
    "plots[2].set_xlabel('Transaction Amount')\n",
    "plots[3].set_xlabel('Transaction Amount')\n",
    "\n",
    "#setting y label\n",
    "plots[1].set_ylabel('Number of transactions')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Plot the 'trans_hour' feature\n",
    "sns.countplot(x='trans_hour', data=df, ax=ax[0])\n",
    "ax[0].set_title(\"Transaction Hour\")\n",
    "\n",
    "# Plot the 'trans_day_of_week' feature\n",
    "sns.countplot(x='trans_day_of_week', data=df, ax=ax[1])\n",
    "ax[1].set_title(\"Transaction Day of Week\")\n",
    "\n",
    "# Plot the 'trans_year_month' feature\n",
    "sns.countplot(x='trans_year_month', data=df, ax=ax[2])\n",
    "ax[2].set_title(\"Transaction Year-Month\")\n",
    "\n",
    "# Adjust x-axis tick labels for better readability\n",
    "for i in range(3):\n",
    "    ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=30)\n",
    "\n",
    "# Hide the last subplot if unused\n",
    "fig.delaxes(ax[3])\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#year_month vs number of transactions\n",
    "df_timeline01 = df.groupby(df['trans_year_month'])[['trans_num','cc_num']].nunique().reset_index()\n",
    "df_timeline01.columns = ['year_month','num_of_transactions','customers']\n",
    "df_timeline01\n",
    "x = np.arange(0,len(df_timeline01),1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "ax.plot(x,df_timeline01['num_of_transactions'])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_timeline01['year_month'])\n",
    "\n",
    "ax.set_xlabel('Year Month')\n",
    "ax.set_ylabel('Num of Transactions')\n",
    "plt.show()\n",
    "df_fraud_transactions = df[df['is_fraud']==1]\n",
    "\n",
    "df_timeline02 = df_fraud_transactions.groupby(df_fraud_transactions['trans_year_month'])[['trans_num','cc_num']].nunique().reset_index()\n",
    "df_timeline02.columns = ['year_month','num_of_fraud_transactions','fraud_customers']\n",
    "df_timeline02\n",
    "x = np.arange(0,len(df_timeline02),1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "ax.plot(x,df_timeline02['fraud_customers'])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_timeline02['year_month'])\n",
    "\n",
    "ax.set_xlabel('Year Month')\n",
    "ax.set_ylabel('Number of Fraud customers')\n",
    "plt.show()\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plotting gender demographic with respect to transactions\n",
    "ax1 = plt.subplot(2, 1, 1)  # Top plot spanning full row\n",
    "sns.countplot(x='gender', data=df, ax=ax1)\n",
    "ax1.set_title(\"Gender Demographic with Respect to Transactions\")\n",
    "\n",
    "# Plotting transactions over time with respect to gender\n",
    "ax2 = plt.subplot(2, 3, 4)  # Bottom left\n",
    "sns.countplot(x='trans_hour', hue='gender', data=df, ax=ax2)\n",
    "ax2.set_title(\"Transactions by Hour with Gender\")\n",
    "\n",
    "ax3 = plt.subplot(2, 3, 5)  # Bottom middle\n",
    "sns.countplot(x='trans_day_of_week', hue='gender', data=df, ax=ax3)\n",
    "ax3.set_title(\"Transactions by Day of Week with Gender\")\n",
    "\n",
    "ax4 = plt.subplot(2, 3, 6)  # Bottom right\n",
    "sns.countplot(x='trans_year_month', hue='gender', data=df, ax=ax4)\n",
    "ax4.set_title(\"Transactions by Year-Month with Gender\")\n",
    "\n",
    "# Rotate x-axis tick labels for all subplots\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#creating the 'gender' distributed dataframe\n",
    "df_gender = df[['gender','trans_num']].groupby(['gender']).count().reset_index()\n",
    "df_gender.columns = ['Gender', 'gender_count']\n",
    "\n",
    "#creating gender-fraud distribution\n",
    "df_fraud_gender = df[['gender','trans_num', 'is_fraud']].groupby(['gender','is_fraud']).count().reset_index()\n",
    "df_fraud_gender.columns = ['Gender', 'is_fraud', 'Transaction Count']\n",
    "\n",
    "df_fraud_gender = df_fraud_gender.merge(df_gender[['Gender', 'gender_count']], how='inner', on='Gender')\n",
    "\n",
    "df_fraud_gender['Transaction percentage'] = (df_fraud_gender['Transaction Count']/df_fraud_gender['gender_count'])*100\n",
    "\n",
    "df_fraud_gender\n",
    "sns.barplot(data=df_fraud_gender, y='Transaction Count', x='Gender', hue='is_fraud')\n",
    "\n",
    "plt.show()\n",
    "#let us first bin the age feature\n",
    "for i in range(len(df.age)):\n",
    "  if df.age[i] <= 30:\n",
    "    df.age[i] = '< 30'\n",
    "  elif df.age[i] > 30 and df.age[i] <= 45:\n",
    "    df.age[i] = '30-45'\n",
    "  elif df.age[i] > 45 and df.age[i] <= 60:\n",
    "    df.age[i] = '46-60'\n",
    "  elif df.age[i] > 60 and df.age[i] <= 75:\n",
    "    df.age[i] = '61-75'\n",
    "  else:\n",
    "    df.age[i] = '> 75'\n",
    "\n",
    "df.age.head()\n",
    "# Ensure age is treated as a categorical variable if it's not already\n",
    "df['age'] = df['age'].astype('category')\n",
    "\n",
    "# Plotting the age feature in the data\n",
    "plot = sns.countplot(x='age', data=df)\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#constructing the age-transaction count distribution\n",
    "df_age = df[['age','trans_num']].groupby(['age']).count().reset_index()\n",
    "df_age.columns = ['age', 'age_count']\n",
    "\n",
    "#creating the age-fraud distribution\n",
    "df_fraud_age = df[['age', 'trans_num', 'is_fraud']].groupby(['age','is_fraud']).count().reset_index()\n",
    "df_fraud_age.columns = ['age', 'is_fraud', 'Transaction count']\n",
    "\n",
    "df_fraud_age = df_fraud_age.merge(df_age[['age', 'age_count']], how='inner', on='age')\n",
    "\n",
    "df_fraud_age['Transaction percentage'] = (df_fraud_age['Transaction count']/df_fraud_age['age_count'])*100\n",
    "\n",
    "df_fraud_age\n",
    "sns.barplot(data=df_fraud_age, y='Transaction count', x='age', hue='is_fraud')\n",
    "\n",
    "plt.show()\n",
    "df.state.nunique()\n",
    "# Create a DataFrame for fraud transaction counts by state\n",
    "train_state_fraud = df.groupby(['state']).sum('is_fraud').sort_values('is_fraud', ascending=False)\n",
    "train_state_fraud.reset_index(level=0, inplace=True)\n",
    "train_state_fraud = train_state_fraud.sort_values('state', ascending=True)\n",
    "train_state_fraud.reset_index(inplace=True)\n",
    "train_state_fraud.drop(columns='index', inplace=True)\n",
    "\n",
    "# Create a DataFrame for non-fraud transaction counts by state\n",
    "train_state_non_fraud_counts = df.groupby('state').agg(non_fraud_count=('is_fraud', lambda x: (x == 0).sum()))\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create subplots (1 row, 2 columns)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, \n",
    "    subplot_titles=['Fraud Transactions by State', 'Non-Fraud Transactions by State'], \n",
    "    specs=[[{\"type\": \"choropleth\"}, {\"type\": \"choropleth\"}]]\n",
    ")\n",
    "\n",
    "# First plot: Fraud transactions\n",
    "fig.add_trace(go.Choropleth(\n",
    "    locations=train_state_fraud['state'], \n",
    "    text=train_state_fraud['state'],\n",
    "    z=train_state_fraud['is_fraud'].astype(float), \n",
    "    locationmode='USA-states', \n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(title=\"is_fraud\", x=0.45)  # Position colorbar next to the first graph\n",
    "), row=1, col=1)\n",
    "\n",
    "# Second plot: Non-fraud transactions\n",
    "fig.add_trace(go.Choropleth(\n",
    "    locations=train_state_non_fraud_counts.index, \n",
    "    text=train_state_non_fraud_counts.index,\n",
    "    z=train_state_non_fraud_counts['non_fraud_count'].astype(float), \n",
    "    locationmode='USA-states', \n",
    "    colorscale='Blues',\n",
    "    colorbar=dict(title=\"non_fraud\", x=1.0)  # Position colorbar next to the second graph\n",
    "), row=1, col=2)\n",
    "\n",
    "# Set the geo scope for each subplot\n",
    "fig.update_geos(scope='usa', row=1, col=1)\n",
    "fig.update_geos(scope='usa', row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Fraud vs Non-Fraud Transactions by State\",\n",
    "    title_x=0.5,             # Center title\n",
    "    title_xanchor='center'   # Anchor title in the center\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "\n",
    "#fetching states with high transaction frequecy\n",
    "high_trans_states = df.state.value_counts().head(20).index.tolist()\n",
    "print(high_trans_states)\n",
    "100*df[df.state.isin(high_trans_states)].state.value_counts(normalize=True)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plotting state feature\n",
    "plot = sns.countplot(x='state', data=df, ax=ax)\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#constructing the state-transaction count distribution\n",
    "df_state = df[['state','trans_num']].groupby(['state']).count().reset_index()\n",
    "df_state.columns = ['state', 'state_count']\n",
    "\n",
    "#creating the state-fraud distribution\n",
    "df_fraud_state = df[['state', 'trans_num', 'is_fraud']].groupby(['state','is_fraud']).count().reset_index()\n",
    "df_fraud_state.columns = ['state', 'is_fraud', 'Transaction count']\n",
    "\n",
    "df_fraud_state = df_fraud_state.merge(df_state[['state', 'state_count']], how='inner', on='state')\n",
    "\n",
    "df_fraud_state['Transaction percentage'] = (df_fraud_state['Transaction count']/df_fraud_state['state_count'])*100\n",
    "\n",
    "#viewing the top 20 states with high fraudulent transactions\n",
    "df_fraud_state[df_fraud_state['is_fraud'] == 1].sort_values(by = ['Transaction percentage'], ascending=False).head(20)\n",
    "#states with more than 75% fraudulent transactions\n",
    "print('state with more than 75% fraudulent transactions:\\n')\n",
    "print(df_fraud_state.loc[(df_fraud_state.is_fraud == 1) & (df_fraud_state['Transaction percentage'] >= 75)].state)\n",
    "print('number of cities: ',df.city.nunique())\n",
    "print('number of zip codes: ',df.zip.nunique())\n",
    "high_trans_cities = df.city.value_counts().head(20).index.tolist()\n",
    "high_trans_zips = df.zip.value_counts().head(20).index.tolist()\n",
    "print('high frequencies cities: ', high_trans_cities)\n",
    "print('high frequencies zip codes: ', high_trans_zips)\n",
    "df.category.value_counts(normalize=True)\n",
    "# Plotting the category-wise counts\n",
    "plot = sns.countplot(x='category', data=df)\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting the number of fraudulent transactions in each category\n",
    "plot = sns.countplot(x='category', hue='is_fraud', data=df)\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#constructing the category-transaction count distribution\n",
    "df_category = df[['category','trans_num']].groupby(['category']).count().reset_index()\n",
    "df_category.columns = ['category', 'category_count']\n",
    "\n",
    "#creating the zip-fraud distribution\n",
    "df_fraud_category = df[['category', 'trans_num', 'is_fraud']].groupby(['category','is_fraud']).count().reset_index()\n",
    "df_fraud_category.columns = ['category', 'is_fraud', 'Transaction count']\n",
    "\n",
    "df_fraud_category = df_fraud_category.merge(df_category[['category', 'category_count']], how='inner', on='category')\n",
    "\n",
    "df_fraud_category['Transaction percentage'] = (df_fraud_category['Transaction count']/df_fraud_category['category_count'])*100\n",
    "\n",
    "#viewing the top categories with high fraudulent transaction volumes\n",
    "df_fraud_category[df_fraud_category['is_fraud'] == 1].sort_values(by = ['Transaction percentage'], ascending=False)\n",
    "#categories with more than one percent fraudulent transactions\n",
    "df_fraud_category.loc[(df_fraud_category.is_fraud == 1) & (df_fraud_category['Transaction percentage'] >= 1)].category\n",
    "df_fraud_category.head()\n",
    "df.merchant.nunique()\n",
    "high_trans_merchants = df.merchant.value_counts().head(20).index.tolist()\n",
    "high_trans_merchants\n",
    "# Plotting the top merchants with high transaction volumes\n",
    "plot = sns.countplot(x='merchant', data=df[df['merchant'].isin(high_trans_merchants)])\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#constructing the merchant-transaction count distribution\n",
    "df_merchant = df[['merchant','trans_num']].groupby(['merchant']).count().reset_index()\n",
    "df_merchant.columns = ['merchant', 'merchant_count']\n",
    "\n",
    "#creating the zip-fraud distribution\n",
    "df_fraud_merchant = df[['merchant', 'trans_num', 'is_fraud']].groupby(['merchant','is_fraud']).count().reset_index()\n",
    "df_fraud_merchant.columns = ['merchant', 'is_fraud', 'Transaction count']\n",
    "\n",
    "df_fraud_merchant = df_fraud_merchant.merge(df_merchant[['merchant', 'merchant_count']], how='inner', on='merchant')\n",
    "\n",
    "df_fraud_merchant['Transaction percentage'] = (df_fraud_merchant['Transaction count']/df_fraud_merchant['merchant_count'])*100\n",
    "\n",
    "#viewing the top 20 merchant with high fraudulent transaction volumes\n",
    "df_fraud_merchant[df_fraud_merchant['is_fraud'] == 1].sort_values(by = ['Transaction percentage'], ascending=False).head(20)\n",
    "\n",
    "#number of merchants with more than one percent fraudulent transactions\n",
    "df_fraud_merchant.loc[(df_fraud_merchant.is_fraud == 1) & (df_fraud_merchant['Transaction percentage'] >= 2)].merchant\n",
    "#one-hot encoding the category variable\n",
    "category_onehot = pd.get_dummies(df.category, prefix='category', drop_first=True)\n",
    "#one-hot encoding the gender variable\n",
    "gender_onehot = pd.get_dummies(df.gender, prefix='gender', drop_first=True)\n",
    "#one-hot encoding the day_of_week variable\n",
    "day_of_week_onehot = pd.get_dummies(df.trans_day_of_week, prefix='day', drop_first=True)\n",
    "#one-hot encoding the age variable\n",
    "age_onehot = pd.get_dummies(df.age, prefix='age', drop_first=True)\n",
    "df1 = pd.concat([df, category_onehot,gender_onehot,day_of_week_onehot,age_onehot], axis=1)\n",
    "\n",
    "df1.head()\n",
    "df1.drop([ 'cc_num', 'trans_num'], axis=1, inplace=True)\n",
    "print(df1.shape)\n",
    "df1.columns\n",
    "df1.drop(['merchant','street','city','state','job',\n",
    "          'category','gender','trans_day_of_week',\n",
    "          'age'],axis=1, inplace=True)\n",
    "df1.columns\n",
    "df1.info()\n",
    "df1 = df1.drop(columns=['trans_year_month'])\n",
    "#let us now check the correlations between the columns\n",
    "df_random_under_corr = df1.corr()\n",
    "#plotting the correlation heatplot\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(df_random_under_corr)\n",
    "plt.show()\n",
    "#function to return highly correlated column above a threshold\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # This set stores the highly correlated columns\n",
    "    corr_matrix = dataset.corr() #correlation matrix\n",
    "    #traversing the correlation matrix\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if corr_matrix.iloc[i,j] > threshold:\n",
    "                colname = corr_matrix.columns[i] #selecting columns above threshold\n",
    "                col_corr.add(colname) #adding columns to set\n",
    "    return col_corr\n",
    "#let us get the features with correlation above 85%\n",
    "corr_features = correlation(df1,0.85)\n",
    "corr_features\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df1.corr()\n",
    "\n",
    "# Display the matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df1.corr().abs()\n",
    "\n",
    "# Select the upper triangle of the matrix\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation above the threshold\n",
    "high_corr = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.8)]\n",
    "print(\"Highly correlated features:\", high_corr)\n",
    "\n",
    "df2=df1\n",
    "df2.info()\n",
    "df2.head()\n",
    "print(df1.info())\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df1.drop(columns=['is_fraud'])\n",
    "y = df1['is_fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Initialize and train the model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Rank features by importance\n",
    "importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
    "print(importances)\n",
    "\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# # Rank features by importance\n",
    "# importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
    "# importances = importances.sort_values(by='Importance', ascending=False)\n",
    "# print(importances)\n",
    "\n",
    "# # Drop features with low importance\n",
    "# low_importance_features = importances[importances['Importance'] < 0.01]['Feature']\n",
    "# df1 = df1.drop(columns=low_importance_features)\n",
    "\n",
    "# Drop features with low importance\n",
    "low_importance_features = importances[importances['Importance'] < 0.01]['Feature']\n",
    "df1 = df1.drop(columns=low_importance_features)\n",
    "df1.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
